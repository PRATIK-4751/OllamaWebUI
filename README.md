<table>
  <tr>
    <td><img src="public/assets/ai-avatar.jpg" width="80" style="border-radius: 20px;"></td>
    <td><h1>Ollama WebUI ‚Äî Renaissance Edition</h1></td>
  </tr>
</table>

A beautiful, high-aesthetic web interface for your local Ollama models. Fast, private, and supercharged with advanced features.

## üñºÔ∏è Gallery

### Home Page
![Home Page](public/assets/home.png)

### Vision Capabilities
![Vision](public/assets/vision.png)

### Chat Interface
![Chat Section](public/assets/textinterface.png)

## ‚ú® Features

- **Chat with any Ollama model** ‚Äî Full support for text, vision, and multimodal models.
- **Web Search** ‚Äî Real-time search powered by DuckDuckGo (toggleable).
- **Image Search** ‚Äî Visual results directly in your chat flow.
- **PDF Intelligence** ‚Äî Upload PDFs and perform instant Q&A (server-side parsing).
- **URL Fetching** ‚Äî Extract content from any website for AI context.
- **Voice & TTS** ‚Äî Speech-to-text input and "Read Aloud" voice responses.
- **Prompt Templates** ‚Äî Pre-defined high-quality prompts to get you started.
- **100% Private** ‚Äî Runs entirely on your local machine.

## üöÄ Quick Start (Docker)

Run everything with a single command:

```bash
docker-compose up -d
```

- **Frontend**: [http://localhost:3000](http://localhost:3000)
- **Backend**: [http://localhost:8000](http://localhost:8000)

> Pre-built image: `docker pull lucifero19/ollama-webui:latest`

## ‚öôÔ∏è Requirements

**Ollama** must be running with CORS enabled:

```powershell
# Windows
$env:OLLAMA_ORIGINS="*"; ollama serve

# Mac / Linux
OLLAMA_ORIGINS="*" ollama serve
```

## üõ†Ô∏è Local Development

```bash
# Install & Run Frontend
npm install
npm run dev

# Install & Run Backend (in /backend)
pip install -r requirements.txt
uvicorn main:app --reload --port 8000
```

## ‚öñÔ∏è License

MIT
