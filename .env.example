# ===================================
# Ollama WebUI Configuration
# ===================================
# Copy this file to .env and customize!

# ----- Ollama Connection -----
# URL of your local Ollama instance
# If deploying to Vercel, users need Ollama running locally with:
#   OLLAMA_ORIGINS=* ollama serve
VITE_OLLAMA_URL=http://localhost:11434

# ----- Model Configuration -----
VITE_DEFAULT_MODEL=llava:7b

# ----- UI Branding -----
VITE_APP_TITLE=Ollama WebUI
VITE_APP_SUBTITLE=Your local AI assistant â€” fast, private, and powerful

# ----- Logo & Avatars -----
# Path relative to /public, or a full URL
VITE_LOGO_IMAGE=/assets/logo.jpg
VITE_AI_AVATAR=/assets/ai-avatar.jpg
VITE_USER_AVATAR=/assets/user-avatar.jpg

# ----- Background Images -----
# Different backgrounds per theme! Replace with your own art.
# Included defaults: Renaissance/Baroque classical art
VITE_BG_DARK=/assets/bg-dark.png
VITE_BG_LIGHT=/assets/bg-light.png

# ----- Theme Colors -----
# Primary accent color (used for buttons, links, active states)
VITE_PRIMARY_COLOR=#dc2626
# Options: "dark" | "light" | "system"
VITE_DEFAULT_THEME=dark

# ----- Feature Toggles -----
VITE_ENABLE_IMAGE_UPLOAD=true
VITE_ENABLE_CHAT_SEARCH=true
VITE_SHOW_MODEL_INFO=true

# ----- Welcome Screen -----
VITE_FEATURE_1_LABEL=Lightning Fast
VITE_FEATURE_2_LABEL=Smart Responses
VITE_FEATURE_3_LABEL=100% Private
